Video Link : https://youtu.be/1Yw9sC0PNwY


Feature scaling is a **data preprocessing technique** in machine learning where you **adjust the range and distribution of numerical features** so that all features contribute equally to the model.

It ensures that **no feature dominates another** simply because of its scale (unit size).

***

# ‚≠ê **What is Feature Scaling?**

Feature scaling is the process of **normalizing or standardizing numerical values** so they fall within a specific scale (like 0‚Äì1) or distribution (like mean=0, std=1).

### Why?

Machine learning algorithms **work better and converge faster** when input features have similar scales.

***

# üéØ **Why Feature Scaling Is Important**

Many algorithms rely on **distance, gradient optimization, or variance**, such as:

| Algorithm                                  | Needs Feature Scaling?   |
| ------------------------------------------ | ------------------------ |
| KNN                                        | ‚úÖ Yes (distance-based)   |
| K-Means                                    | ‚úÖ Yes                    |
| SVM                                        | ‚úÖ Yes                    |
| Logistic Regression                        | Often Yes                |
| Neural Networks                            | Yes (faster convergence) |
| PCA                                        | Yes (variance-dependent) |
| Tree-based models (Random Forest, XGBoost) | ‚ùå Not needed             |

***

# üß† **Intuition Example**

Suppose you have two features:

| Feature | Range          |
| ------- | -------------- |
| Age     | 20‚Äì60          |
| Income  | 10,000‚Äì500,000 |

Income has much bigger values ‚Üí it dominates the model.

After scaling both features to the same range, the model treats them **equally**.

***

# üß© **Types of Feature Scaling (With Examples)**

***

# 1Ô∏è‚É£ **Normalization (Min‚ÄëMax Scaling)**

Scales values to a **fixed range**, usually **0 to 1**.

### Formula:

$$
x' = \frac{x - x_{min}}{x_{max} - x_{min}}
$$

### Example

Original values: `[10, 20, 30]`  
Min=10, Max=30

Scaled:

    0.0, 0.5, 1.0

### Python

```
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df[['age','income']] = scaler.fit_transform(df[['age','income']])
```

### Best for:

*   Neural networks
*   Distance algorithms (KNN, K-Means)

***

# 2Ô∏è‚É£ **Standardization (Z‚ÄëScore Scaling)**

Transforms data such that **mean = 0** and **std = 1**.

### Formula:

$$
x' = \frac{x - \mu}{\sigma}
$$

### Example:

Original: `[10, 20, 30]`  
Mean=20, Std=8.16

Scaled:

    [-1.22, 0, 1.22]

### Python:

```
from sklearn.preprocessing import StandardScaler

df[['age','salary']] = StandardScaler().fit_transform(df[['age','salary']])
```

### Best for:

*   Linear Regression
*   Logistic Regression
*   SVM
*   PCA

***

# 3Ô∏è‚É£ **Robust Scaling (Handles Outliers)**

Uses **median** and **IQR** instead of mean/std.

### Formula:

$$
x' = \frac{x - median}{IQR}
$$

### Python:

```
from sklearn.preprocessing import RobustScaler
df[['income']] = RobustScaler().fit_transform(df[['income']])
```

### Use when:

*   Your data contains **outliers**
*   Example: Salary, house prices

***

# 4Ô∏è‚É£ **MaxAbs Scaling**

Scales data to **\[-1, 1]** without shifting to mean=0.

### Python:

```
from sklearn.preprocessing import MaxAbsScaler
df[['x']] = MaxAbsScaler().fit_transform(df[['x']])
```

### Use when:

*   Data already centered at zero
*   Sparse data

***

# 5Ô∏è‚É£ **Unit Vector Scaling (Vector Normalization)**

Scales **each sample** (row) to have length = 1.

### Used in:

*   Text data (TF‚ÄëIDF)
*   NLP embeddings

### Python:

```
from sklearn.preprocessing import Normalizer
X = Normalizer().fit_transform(X)
```

***

# üß† **Quick Comparison Table**

| Method              | When to Use          | Sensitive to Outliers? | Range         |
| ------------------- | -------------------- | ---------------------- | ------------- |
| **Min-Max**         | Neural networks, KNN | Yes                    | \[0,1]        |
| **Standardization** | Most ML models       | Yes                    | Mean=0, Std=1 |
| **Robust Scaling**  | Outlier-heavy data   | No                     | Centered      |
| **MaxAbs**          | Sparse data          | Yes                    | \[-1,1]       |
| **Normalizer**      | Text/NLP             | No                     | Unit length   |

***

# üé¨ Real-World Example (ML Model)

Suppose you're building a **loan approval model** with features:

*   Age (20‚Äì60)
*   Salary (20,000‚Äì200,000)
*   Loan Amount (50,000‚Äì1,000,000)

If you don't scale:

*   Loan amount dominates the gradient
*   Model converges poorly
*   Decision boundary becomes skewed

After scaling:

*   All features contribute equally
*   Training becomes stable
*   Accuracy improves

***

# üìå **Normalization vs Standardization ‚Äî Quick Comparison**

| Feature                       | Normalization             | Standardization         |
| ----------------------------- | ------------------------- | ----------------------- |
| Range                         | 0 to 1                    | Mean = 0, Std = 1       |
| Preserves distribution shape? | ‚úî Yes                     | ‚ùå No                    |
| Sensitive to outliers         | ‚ùå Very sensitive          | ‚úî More robust           |
| Best for                      | Neural Nets, KNN, K-Means | Linear models, SVM, PCA |

***

# üî• Summary

Feature scaling is essential to:

*   Improve model performance
*   Help optimization algorithms converge faster
*   Prevent large-valued features from dominating

Most commonly used:

*   **StandardScaler** (default choice)
*   **MinMaxScaler** (for deep learning)
*   **RobustScaler** (for outliers)



