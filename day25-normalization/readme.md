Video Link: https://youtu.be/eBrGyuA2MIg


# ğŸŒŸ **What is Normalization in Machine Learning?**

**Normalization** is a feature scaling technique that transforms numerical values into a **common scale**, typically **0 to 1**.

It keeps the **shape of the data distribution**, but compresses it so no feature dominates others.

Normalization is also called:

*   **Minâ€‘Max Scaling**
*   **Rescaling**
*   **0â€“1 Scaling**

***

# ğŸ”¢ **Normalization (Minâ€“Max Scaling) Formula**

$$
x' = \frac{x - x_{min}}{x_{max} - x_{min}}
$$

Where:

*   $$x$$ = original value
*   $$x_{min}$$ = minimum value of the feature
*   $$x_{max}$$ = maximum value of the feature
*   $$x'$$ âˆˆ $$[0,1]$$

***

# ğŸ¯ Why Do We Use Normalization?

Normalization is used when:

*   Features have **different scales**
*   Using distanceâ€‘based models:
    âœ” Kâ€‘Means  
    âœ” KNN  
    âœ” Neural networks  
    âœ” Logistic Regression

Example:
If **income** = 200000 and **age** = 30,
the model may consider *income 6000 times more important* unless scaled.

***

# ğŸ§  **Simple Example (Step-by-Step Calculation)**

Consider the feature:

    Salaries = [20,000, 50,000, 100,000]

### Step 1: Identify min & max

*   Min = 20,000
*   Max = 100,000

### Step 2: Apply Min-Max formula

For value = 50,000:

$$
x' = \frac{50000 - 20000}{100000 - 20000}
$$

$$
x' = \frac{30000}{80000} = 0.375
$$

### ğŸ“Œ Final Normalized Values

| Original | Normalized |
| -------- | ---------- |
| 20,000   | 0.0        |
| 50,000   | 0.375      |
| 100,000  | 1.0        |

Now the entire feature lies in **\[0,1]**.

***

# ğŸ–¼ Visual Intuition (Conceptual)

Before normalization:

    20,000 â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 100,000

After normalization:

    0.0 â€”â€”â€”â€”â€”â€” 0.3 â€”â€”â€”â€”â€”â€” 1.0

Same order, smaller scale.

***

# ğŸ§ª **Python Example: Minâ€‘Max Normalization**

```
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

df = pd.DataFrame({
    'salary': [20000, 50000, 80000, 100000]
})

scaler = MinMaxScaler()
df_scaled = scaler.fit_transform(df[['salary']])

print(df_scaled)
```

### Output:

    [[0.   ]
     [0.375]
     [0.75 ]
     [1.   ]]

***

# ğŸ§© When Should You Use Normalization?

### Use normalization when:

âœ“ You use **distance-based** models  
âœ“ Data has **different units** (km, kg, $, years)  
âœ“ Neural networks â€” helps faster convergence  
âœ“ Using gradient descent-based models

### Do NOT normalize when:

âœ— Using **tree-based** models  
(Random Forest, XGBoost, LightGBM)

Trees do not care about scale.


# ğŸŒ Realâ€‘World Example: Normalizing Image Pixels

Images have pixel values:

    0â€“255

Deep learning models **always normalize** images:

$$
x' = \frac{x}{255}
$$

This speeds up training and stabilizes gradients.

Example:

```
image = image / 255.0
```

***

# ğŸ Final Summary

**Normalization** rescales features to the range **\[0,1]**, ensuring all features contribute equally.

### You should use it when:

*   Features have different units
*   Using KNN, Kâ€‘Means, Neural Networks, Logistic Regression
*   Preparing image data for CNNs

### Formula:

$$
x' = \frac{x - x_{min}}{x_{max} - x_{min}}
$$

***


